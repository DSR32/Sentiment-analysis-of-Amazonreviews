{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DsngXcNMhgWc"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EzSII-tbhx5s"
   },
   "outputs": [],
   "source": [
    "#function that loads a lexicon of positive words to a set and returns the set\n",
    "def loadLexicon(fname):\n",
    "    newLex=set()\n",
    "    lex_conn=open(fname)\n",
    "    \n",
    "    #add every word in the file to the set\n",
    "    for line in lex_conn:\n",
    "        newLex.add(line.strip())# remember to strip to remove the lin-change character\n",
    "    lex_conn.close()\n",
    "\n",
    "    return newLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gZvWT50AhzG5"
   },
   "outputs": [],
   "source": [
    "def parse(input_path, index1, index2):\n",
    "    \n",
    "    df = pd.read_csv(input_path +'amazonreviews.csv', header = None)\n",
    "    \n",
    "    review1 = df.iloc[index1, 0]\n",
    "    review2 = df.iloc[index2, 0]\n",
    "    \n",
    "    #load the positive and negative lexicons into sets\n",
    "    posLex=loadLexicon(input_path +'positive-words.txt')\n",
    "    negLex=loadLexicon(input_path +'negative-words.txt')\n",
    "    \n",
    "    \n",
    "        \n",
    "    sentences=sent_tokenize(review1) # split the review into sentences \n",
    "    \n",
    "    df_list = []\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        words=word_tokenize(sentence) # split the review into words\n",
    "    \n",
    "        tagged_words=nltk.pos_tag(words) # POS tagging for the words in the sentence\n",
    "    \n",
    "        \n",
    "        nouns = []\n",
    "        #posCount = []\n",
    "        #negCount = []\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        for tagged_word in tagged_words:\n",
    "            if tagged_word[1].startswith('NN'):\n",
    "                if len(tagged_word[0]) >3:\n",
    "                    nouns.append(tagged_word[0])\n",
    "                \n",
    "            if tagged_word[1].startswith('JJ'):\n",
    "                if tagged_word[0] in posLex:\n",
    "                    pos = pos+1\n",
    "                if tagged_word[0] in negLex:\n",
    "                    neg = neg+1\n",
    "        \n",
    "        df1 = pd.DataFrame()\n",
    "        df1['nouns'] = nouns\n",
    "        df1['posCount'] = pos\n",
    "        df1['negCount'] = neg\n",
    "        \n",
    "        df_list.append(df1)\n",
    "    \n",
    "    \n",
    "    df_result1 = pd.concat(df_list, 0)\n",
    "    \n",
    "    \n",
    "    df_result1_ = df_result1.groupby(['nouns']).sum()\n",
    "    \n",
    "    df_result1_['sentiment'] = np.where(df_result1_['posCount'] > df_result1_['negCount'], 'Positive', 'Negative')\n",
    "    df_result1_['sentiment'] = np.where(df_result1_['posCount'] == df_result1_['negCount'], 'Neutral', df_result1_['sentiment'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    sentences2=sent_tokenize(review2) # split the review into sentences \n",
    "    \n",
    "    df_list2 = []\n",
    "    for sentence in sentences2:\n",
    "        \n",
    "        words=word_tokenize(sentence) # split the review into words\n",
    "    \n",
    "        tagged_words=nltk.pos_tag(words) # POS tagging for the words in the sentence\n",
    "    \n",
    "        \n",
    "        nouns = []\n",
    "        #posCount = []\n",
    "        #negCount = []\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        for tagged_word in tagged_words:\n",
    "            if tagged_word[1].startswith('NN'):\n",
    "                if len(tagged_word[0]) >3:\n",
    "                    nouns.append(tagged_word[0])\n",
    "                \n",
    "            if tagged_word[1].startswith('JJ'):\n",
    "                if tagged_word[0] in posLex:\n",
    "                    pos = pos+1\n",
    "                if tagged_word[0] in negLex:\n",
    "                    neg = neg+1\n",
    "        \n",
    "        df2 = pd.DataFrame()\n",
    "        df2['nouns'] = nouns\n",
    "        df2['posCount'] = pos\n",
    "        df2['negCount'] = neg\n",
    "        \n",
    "        df_list2.append(df2)\n",
    "    \n",
    "    \n",
    "    df_result2 = pd.concat(df_list2, 0)\n",
    "    \n",
    "    \n",
    "    df_result2_ = df_result2.groupby(['nouns']).sum()\n",
    "    \n",
    "    df_result2_['sentiment'] = np.where(df_result2_['posCount'] > df_result2_['negCount'], 'Positive', 'Negative')\n",
    "    df_result2_['sentiment'] = np.where(df_result2_['posCount'] == df_result2_['negCount'], 'Neutral', df_result2_['sentiment'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_result1_ = df_result1_.reset_index()\n",
    "    df_result2_ = df_result2_.reset_index()\n",
    "    \n",
    "    \n",
    "    opp_opinion = []\n",
    "    \n",
    "    df_result1_1 = df_result1_[df_result1_['sentiment'] != 'Neutral']\n",
    "    df_result2_2 = df_result2_[df_result2_['sentiment'] != 'Neutral']\n",
    "    \n",
    "    \n",
    "    for i in range(0, df_result1_1.shape[0]):\n",
    "        current_noun = df_result1_1['nouns'].iloc[i]\n",
    "        sentiment1 = df_result1_1['sentiment'].iloc[i]\n",
    "        \n",
    "    \n",
    "    \n",
    "        if current_noun in df_result2_2['nouns'].tolist():\n",
    "            temp = df_result2_2[df_result2_2['nouns'] == current_noun]\n",
    "            sentiment2 = temp['sentiment'].iloc[0]\n",
    "            \n",
    "            \n",
    "            if sentiment2 != sentiment1:\n",
    "                opp_opinion.append(current_noun)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return opp_opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RM1zp8iViFEz"
   },
   "outputs": [],
   "source": [
    "input_path = \"C:/Users/praty/Downloads/Text Mining/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bose', 'Sony', 'collection', 'headband']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(input_path, 3, 4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
